In this dissertation, my goal is to present to the readers all of the essence of Neural Machine Translation, through which I discuss how I have contributed to the development of NMT since its birth as a fringe research project in 2014 to its well-established status as a mainstream approach for machine translation including commercial deployments in 2016. Chapter 1 -- {\it Introduction} walks the readers through the history and fundamentals of machine translation together with drawbacks of existing approaches, leading to the development of NMT. In Chapter 2 -- {\it Background}, I provide readers with all the necessary knowledge to fully understand and build a vanilla NMT, which covers details of language model and recurrent neural network, a basic building block for NMT. Several key highlights in this chapter include (a) a complete derivation for the gradients of LSTM and its backpropagation algorithm in \secref{subsec:LSTM} and (b) the forward and backpropagation steps of multi-layer NMT models in \algo{a:nmt_forward} and \ref{a:nmt_backward}. 

Chapter 3 -- {\it Copy Mechanism} starts discussing my contribution. When I was developing the work for this chapter in 2014,
%as an intern at Google Brain, 
NMT had just started with the seminal work of \newcite{sutskever14}. Despite its potential, NMT models at that time had not been able to surpass phrase-based models and suffered from the limited-vocabulary problem. Specifically, NMT models often use a single \unk{} token to represent all other words not in its vocabulary, but do not know how to handle them at translation time. My proposed copy mechanisms provide simple yet effective ways for alleviating that problem. By learning to align the target \unk{} with words on the source through additional annotations to the training data (no need to modify the models, i.e., we can treat any NMT models as a black box), we can post-process target unknown translations much easier through word dictionary translation and identity copy of source words. This approach provides a further lift to the performance of the vanilla NMT model, allowing us for the first time to build an English-French NMT system that achieves state-of-the-art performance. Looking back, one can think of how we track target unknown words as a special case of the attention mechanism.
%, which has now been the de-facto standard in NMT. 
Still, the idea of copy mechanism remains to be useful until now, especially when adapting seq2seq models to new tasks such as text summarization \cite{gu16,gulcehre16} and semantic parsing \cite{jia16}.

Chapter 4 -- {\it Attention Mechanism} includes my deep exploration of what is now the de factor standard in NMT, the attention mechanism, which improves translation quality for long sentences. At the time of my work, there has been little study in useful architectures for attention-based NMT apart from the introduction of the attention mechanism to NMT in the seminal paper of \newcite{bog15}. My work experiments with various variants of the attention mechanism and analyze the effect of different attentional components. One of the key highlights of my work is the introduction of a simple bilinear attentional function to compare source and target states which has now been widely adopted by many people, such as Harvard NLP\footnote{\url{https://github.com/harvardnlp/seq2seq-attn}}, and across different domains such as reading comprehension \cite{chen16} and dependency parsing \cite{dozat16}. I have also introduced a local attention mechanism that only focuses on a different subset of the source sentence at each time step. While I have demonstrated the effectiveness of the local attention mechanism on translation, I wish that I could have tested it over tasks that involve very long sequences which local attention was designed for. Overall, the result of this work is a new state-of-the-art NMT system for English-German, a harder language pair than English-French, which further convinces people on the superiority of NMT.

Chapter 5 -- {\it Hybrid Models} can be viewed as my continuing effort to completely solve the rare word problem in NMT which was introduced in Chapter 3.
Instead of having separate components to perform data annotation, train NMT models, and post-process, we build a single model that handles unlimited vocabulary. Motivating by the proven word-level seq2seq architecture for NMT and the flexibility of character-based models in handling complex and unknown words, we propose hybrid models translate mostly at the word level and consult the character components for rare words only. The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. This advance leads us to conquer (with state-of-the-art performance) another challenging language pair, English-Czech, in which the target is a highly-inflected language with a complex vocabulary. I have also demonstrated an impressive gain from 2.1 to 11.4 BLEU points over models that already handle unknown words. Recently, there is a new trend of translating at purely subword level \cite{sennrich16sub,gnmt16} in which a segmentation algorithm is run over the data and a black-box NMT model is used in a similar spirit to the copy mechanism. While that new trend works very well for NMT, I think that the hybrid models that I propose will remain useful for new applications where segmentation of units, such as predicates in semantic parsing, is not desirable or when we want to incorporate more complex structures such as semantic representations (instead of sequences of characters) for unknown entities to existing systems.

Chapter 6 -- {\it NMT Future} examines two questions that I think are important to the future of NMT: whether other tasks can be utilized to improve translation and whether NMT models can be compressed. The former question is important because of the fact that the first NMT systems only utilize parallel corpora despite an abundant amount of available data from monolingual and multi-lingual corpora as well as data from related tasks. To answer, 
I demonstrate that translation quality can be improved with data from parsing, image caption, and unsupervised learning. My work motivates subsequent papers in building multi-lingual NMT models \cite{zoph16,firat16,gnmt16multi,ha16}.
 The latter question arises from the indispensable role of mobile devices in society nowadays and the fact that state-of-the-art NMT models are beyond the storage capacity of existing mobile gadgets. With simple pruning schemes, my results show that the parameters of NMT models can be pruned up to 80\% without any loss in performance as long as pruned models are retrained. Subsequent work \cite{kim16distill} combines our proposed pruning approach with knowledge distillation to obtain further gain in model compression. Beside the aforementioned questions, in \secref{sec:outlook}, I cover in-depth the existing research landscape, highlight potential research directions, and speculate on future elements needed to further advance NMT 

Lastly, I am fortunate to have gone through a exciting journey in developing Neural Machine Translation since its early days. I hope that this dissertation will provide useful background and inspiration for future research in building much more advanced NMT models, through which I expect the babelfish to become a reality very soon!
