I have shown that weight pruning with retraining is a highly effective method of compression and regularization on a state-of-the-art NMT system, compressing the model to 20\% of its size with no loss of performance. 
Though I are the first to apply compression techniques to NMT, I obtain a similar degree of compression to other current work on compressing state-of-the-art deep neural networks, with an approach that is simpler than most.
I have found that the absolute size of parameters is of primary importance when choosing which to prune, leading to an approach that is extremely simple to implement, and can be applied to any neural network.
Lastly, I have gained insight into the distribution of redundancy in the NMT architecture.

In terms of future work, including \emph{several} iterations of pruning and retraining would likely improve the compression and performance of my pruning method.
If possible it would be highly valuable to exploit the sparsity of the pruned
models to speed up training and runtime, perhaps through sparse matrix
representations and multiplications (see Section \ref{subsubsec:approach_retraining}).
Though I have found magnitude-based pruning to perform very well, it would be instructive to revisit the original claim that other pruning methods (for example Optimal Brain Damage and Optimal Brain Surgery) are more principled, and perform a comparative study.


