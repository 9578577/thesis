
In this section, I showed that multi-task learning (MTL) can improve the
performance of the attention-free sequence to sequence model of
\citep{sutskever14}.  I found it surprising that training on syntactic
parsing and image caption data improved our translation performance, given
that these 
datasets are orders of magnitude smaller than typical translation
datasets. 
Furthermore, I have established a new {\it state-of-the-art} result in
constituent parsing with an ensemble of multi-task models.
I also showed that the two unsupervised
learning objectives, autoencoder and skip-thought, behave differently in the MTL context
involving translation. I hope that these interesting
findings will motivate future work in utilizing unsupervised data for sequence
to sequence learning.
A criticism of this work is that the sequence to sequence models do not employ
the attention mechanism \citep{bog15}.  I leave the exploration
of MTL with attention for future work.



