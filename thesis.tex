\documentclass[12pt]{report}
\usepackage{suthesis}
%\documentstyle[12pt,suthesis]{report}

% -- Imports --
% (general libraries)
\usepackage{times,latexsym,amsfonts,amssymb,amsmath,graphicx,bbm,rotating}
\usepackage{enumitem,multirow,hhline,stmaryrd,bussproofs,mathtools,siunitx,arydshln}
\usepackage{booktabs,xcolor,csquotes}
% (custom libraries)
\usepackage{fitch}
% (inline references)
\usepackage[square,sort,comma,numbers]{natbib}
% (tikz)
\usepackage{tikz}
\usetikzlibrary{shapes.arrows,chains,positioning,automata,trees,calc}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
% (tikz dependency tree)
\usepackage{tikz-dependency,pifont}
% (print algorithms)
\usepackage[ruled,lined,linesnumbered]{algorithm2e}
% (custom)
\input std-macros.tex
\input macros.tex	
% (paper compilation hacks)
\def\newcite#1{\citet{#1}}
\definecolor{darkblue}{rgb}{0.0,0.0,0.4}

%% Thang
\usepackage{epigraph}
\renewcommand{\epigraphsize}{\small}
\setlength{\epigraphwidth}{0.8\textwidth}
\usepackage{pstricks} % for overlay latex on top of images

% to split on hyphens for long urls
\usepackage{url}
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
% the below works for pdflatex
%\usepackage[hyphens]{url} 
%\usepackage{hyperref}


\definecolor{lightgreen}{HTML}{44B178}
\definecolor{lightblue}{HTML}{3B8CDA}
\newcommand{\error}[1]{{\color{red} #1}}
\newcommand{\ngram}{$n$-gram}
\newcommand{\word}[1]{``#1''}
\newcommand{\unk}{$<$\texttt{unk}$>$}
\newcommand{\sos}{$<$\texttt{sos}$>$}
\newcommand{\eos}{$<$\texttt{eos}$>$}
\newcommand{\eq}[1]{Eq.~(\ref{#1})}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\nlmtext}{neural probabilistic language models}
\newcommand{\nlm}{NPLM}
\newcommand{\nlms}{NPLMs}

% math operators
%\DeclareMathOperator*{\argmax}{\arg\,\max}
%\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\e}{e}
\DeclareMathOperator{\trace}{Tr}
\DeclareMathOperator{\sigm}{sigm}
\DeclareMathOperator{\sigmoid}{sigmoid}
\DeclareMathOperator{\softmax}{softmax}

% math
\newcommand{\MB}[1]{\mbox{\boldmath{$#1$}}} % boldface for matrix and vectors
% derivative fractions
\newcommand{\parfrac}[1]{\frac{\partial}{\partial #1}} 
% fraction derivatives
\newcommand{\fracder}[2]{\frac{\partial #1}{\partial #2}}
% sum and product
\newcommand{\sumkn}{\sum_{k=1}^{n}} %I use this when deriving backprop eqns. n is the LSTM block size and k is a better choice than i because it won't get confused with the input gate
% plus equal
\newcommand{\pluseq}{\mathrel{+}=}
% transpose
\newcommand{\tp}[1]{\MB{#1}^\top}
% round bracket
%\newcommand{\open}[1]{\left(#1\right)}
\newcommand{\paren}[1]{\left(#1\right)}
% norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% grad
\newcommand{\grad}[1]{\nabla #1}

%%% math commands %%%
% variables 
\usepackage{bm}
\newcommand{\x}[1]{\bm{x}_{#1}}
\newcommand{\y}[1]{\bm{y}_{#1}}
\newcommand{\W}[1]{\bm{W}_{#1}}
\newcommand{\hid}[1]{\bm{h}_{#1}}
\newcommand{\inR}[1]{\in \mathbb{R}^{#1}}
\newcommand{\yrange}[2]{y_{\overline{#1,#2}}}
\newcommand{\ytop}[1]{y^{(#1)}}
\newcommand{\thetav}{\bm{\theta}}

% NMT
\newcommand{\tgt}[1]{y_{#1}} % target word
\newcommand{\src}[1]{x_{#1}} % source word
\newcommand{\al}{\MB{a}_{t}} % align
\newcommand{\hi}{\MB{h}_{t}} % hidden top
\newcommand{\his}{\MB{\bar{h}}_{s}} % hidden top source
\newcommand{\hd}[1]{\MB{h}_{#1}} % hidden target 
\newcommand{\hb}[1]{\MB{\bar{h}}_{#1}} % hidden source
\newcommand{\rnn}{\MB{T}_{d \times 2d}} % hidden top
\newcommand{\lstm}{\MB{T}_{4d \times 2d}} % hidden top


%% Abi
\usepackage{amsthm} %for the proof environment
%theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}

% lstm
\newcommand{\ilt}{\MB{i^l_t}}
\newcommand{\flt}{\MB{f^l_t}}
\newcommand{\olt}{\MB{o^l_t}}
\newcommand{\glt}{\MB{\hat{h}^l_t}}
\newcommand{\hlt}{\MB{h^l_t}}
\newcommand{\clt}{\MB{c^l_t}}

\newcommand{\dilt}{\MB{\delta_i^l(t)}}
\newcommand{\dflt}{\MB{\delta_f^l(t)}}
\newcommand{\dolt}{\MB{\delta_o^l(t)}}
\newcommand{\dglt}{\MB{\delta_{\hat{h}}^l(t)}}
\newcommand{\dhlt}{\MB{\delta_h^l(t)}}
\newcommand{\dclt}{\MB{\delta_c^l(t)}}

\newcommand{\zilt}{z_i^l(t)}
\newcommand{\zflt}{z_f^l(t)}
\newcommand{\zolt}{z_o^l(t)}
\newcommand{\zglt}{z_{\hat{h}}^l(t)}
\newcommand{\zhlt}{z_h^l(t)}
\newcommand{\zclt}{z_c^l(t)}


% -- Document --
\begin{document}

% Title
\title{Neural Machine Translation}
\author{Minh-Thang Luong}
\principaladviser{Christopher D. Manning}
\firstreader{Dan Jurafsky}
\secondreader{Andrew Ng}
\thirdreader{Quoc V. Le}

% Preface
\beforepreface
%\input preface.tex
%\input ack.tex
\afterpreface


% -- Sections --
\chapter{Introduction}
\label{c:intro}
\input{1-intro.tex}

\chapter{Background}
\label{c:background}
\input{2-background.tex}

\chapter{Copy Mechanisms}
\label{c:copy}
\input{3-copy.tex}

\chapter{Attention Mechanisms}
\label{c:attention}
\input{4-attention.tex}

\chapter{Hybrid Models}
\label{c:hybrid}
\input{5-hybrid.tex}

\chapter{NMT Future}
\label{c:future}
\input{6-future.tex}

\chapter{Conclusion}
\label{c:conclude}
\input{7-conclude.tex}


% -- Appendix --
\appendix
\chapter{Miscellaneous}
\begin{lemma}
\label{l:diag_mul}
Let $\bm{v}$, $\bm{z}$ be any vectors and $\odot$ be element-wise vector multiplication, we have:
\begin{align}
\diag (\bm{v}) \cdot \bm{z} = \bm{v} \odot \bm{z}
\end{align}
\end{lemma}

\begin{lemma}
Let $l$ be a scalar value that was computed using a vector $\bm{v}$ in which we
already knew how to compute the gradients of $\fracder{l}{\bm{v}}$. Given that
$\bm{v} = f(\bm{Wh})$, chain rules in vector calculus can be applied to arrive at the
following formulae:
\begin{align}
\fracder{s}{\bm{h}} = \tp{\bm{W}}\paren{f'(\bm{Wh}) \odot \fracder{s}{\bm{v}}}
\\
\fracder{s}{\bm{W}} = \paren{f'(\bm{Wh}) \odot \fracder{s}{\bm{v}}}\tp{\bm{h}} 
\end{align}
\end{lemma}

\begin{proof}
Let $\bm{z} = \bm{Wh}$, we have the following derivations: %applying chain rules in vector calculus, 
\begin{align*}
\fracder{s}{\bm{h}} &= \fracder{\bm{z}}{\bm{h}} \cdot \fracder{\bm{v}}{\bm{z}}
\cdot \fracder{s}{\bm{v}} && \text{[Vector calculus chain rules]}\\
&= \fracder{\bm{Wh}}{\bm{h}} \cdot \fracder{f(\bm{z})}{\bm{z}}
\cdot \fracder{s}{\bm{v}} \\
&=\tp{\bm{W}} \cdot \diag \paren{f'(\bm{z})} \cdot \fracder{s}{\bm{v}} \\
&=\tp{\bm{W}}\paren{f'(\bm{Wh}) \odot \fracder{s}{\bm{v}}} &&
\text{[Lemma~\ref{l:diag_mul}]}
\end{align*}

Let $\bm{w}_i^{\top}$ be the i$^{\text{th}}$ row vector of matrix $\bm{W}$ and
$v_i, z_i$ be the i$^{\text{th}}$ elements of vectors $\bm{v}, \bm{z}$, we
have:
\begin{align*}
\fracder{s}{\bm{w}_i} &= \fracder{z_i}{\bm{w}_i} \cdot \fracder{v_i}{z_i}
\cdot \fracder{s}{v_i} && \text{[Vector calculus chain rules]}\\
&= \fracder{\bm{w}_i^{\top}\bm{h}}{\bm{w}_i} \cdot f'(z_i) \cdot \fracder{s}{v_i} \\
&= \bm{h} \cdot f'(z_i) \cdot \fracder{s}{v_i} \\ 
\fracder{s}{\bm{w}_i^{\top}} &= \paren{f'(z_i) \cdot \fracder{s}{v_i}} \cdot
\tp{\bm{h}} && \text{[Tranposing]} \\
\fracder{s}{\bm{W}} &= \paren{f'(\bm{Wh}) \odot \fracder{s}{\bm{v}}}\tp{\bm{h}}
&& \text{[Concatenating row derivatives]}
\end{align*}


\end{proof}

%\chapter{LSTM Backpropagation}
%\input{2-backwardprop}

\bibliographystyle{plainnat}
\bibliography{thesis}
\end{document}
