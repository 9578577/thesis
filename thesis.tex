\documentclass[12pt]{report}
\usepackage{suthesis}
%\documentstyle[12pt,suthesis]{report}

% -- Imports --
% (general libraries)
\usepackage{times,latexsym,amsfonts,amssymb,amsmath,graphicx,bbm,rotating}
\usepackage{enumitem,multirow,hhline,stmaryrd,bussproofs,mathtools,siunitx,arydshln}
\usepackage{booktabs,xcolor,csquotes}
% (custom libraries)
\usepackage{fitch}
% (inline references)
\usepackage[square,sort,comma,numbers]{natbib}
% (tikz)
\usepackage{tikz}
\usetikzlibrary{shapes.arrows,chains,positioning,automata,trees,calc}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
% (tikz dependency tree)
\usepackage{tikz-dependency,pifont}
% (print algorithms)
\usepackage[ruled,lined,linesnumbered]{algorithm2e}
% (custom)
\input std-macros.tex
\input macros.tex	
% (paper compilation hacks)
\def\newcite#1{\citet{#1}}
\definecolor{darkblue}{rgb}{0.0,0.0,0.4}

%% Thang
\usepackage{epigraph}
\renewcommand{\epigraphsize}{\small}
\setlength{\epigraphwidth}{0.8\textwidth}
\usepackage{pstricks} % for overlay latex on top of images

% to split on hyphens for long urls
\usepackage{url}
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
% the below works for pdflatex
%\usepackage[hyphens]{url} 
%\usepackage{hyperref}


\definecolor{lightgreen}{HTML}{44B178}
\definecolor{lightblue}{HTML}{3B8CDA}
\newcommand{\error}[1]{{\color{red} #1}}
\newcommand{\ngram}{$n$-gram}
\newcommand{\word}[1]{``#1''}
\newcommand{\unk}{$<$\texttt{unk}$>$}
\newcommand{\sos}{$<$\texttt{sos}$>$}
\newcommand{\eos}{$<$\texttt{eos}$>$}
\newcommand{\eq}[1]{Eq.~(\ref{#1})}
\newcommand{\lemmaref}[1]{Lemma~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\nlmtext}{neural probabilistic language models}
\newcommand{\nlm}{NPLM}
\newcommand{\nlms}{NPLMs}

% math operators
%\DeclareMathOperator*{\argmax}{\arg\,\max}
%\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\e}{e}
\DeclareMathOperator{\trace}{Tr}
\DeclareMathOperator{\sigm}{sigm}
\DeclareMathOperator{\sigmoid}{sigmoid}
\DeclareMathOperator{\softmax}{softmax}

% math
\newcommand{\MB}[1]{\mbox{\boldmath{$#1$}}} % boldface for matrix and vectors
% derivative fractions
\newcommand{\parfrac}[1]{\frac{\partial}{\partial #1}} 
% fraction derivatives
\newcommand{\fracder}[2]{\frac{\partial #1}{\partial #2}}
% sum and product
\newcommand{\sumkn}{\sum_{k=1}^{n}} %I use this when deriving backprop eqns. n is the LSTM block size and k is a better choice than i because it won't get confused with the input gate
% plus equal
\newcommand{\pluseq}{\mathrel{+}=}
% transpose
\newcommand{\tp}[1]{#1^\top}
% round bracket
%\newcommand{\open}[1]{\left(#1\right)}
\newcommand{\paren}[1]{\left(#1\right)}
% norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% grad
\newcommand{\grad}[1]{\nabla #1}

%%% math commands %%%
% variables 
\usepackage{bm}
\newcommand{\x}[1]{\bm{x}_{#1}}
\newcommand{\y}[1]{\bm{y}_{#1}}
\newcommand{\z}[1]{\bm{z}_{#1}}
\newcommand{\s}[1]{\bm{s}_{#1}}
\newcommand{\prob}[1]{\bm{p}_{#1}}
\newcommand{\der}[1]{d\bm{#1}}
\newcommand{\W}[1]{\bm{W}_{#1}}
\newcommand{\hid}[1]{\bm{h}_{#1}}
\newcommand{\inR}[1]{\in \mathbb{R}^{#1}}
\newcommand{\yrange}[2]{y_{\overline{#1,#2}}}
\newcommand{\ytop}[1]{y^{(#1)}}
\newcommand{\thetav}{\bm{\theta}}

% NMT
\newcommand{\tgt}[1]{y_{#1}} % target word
\newcommand{\src}[1]{x_{#1}} % source word
\newcommand{\al}{\MB{a}_{t}} % align
\newcommand{\hi}{\MB{h}_{t}} % hidden top
\newcommand{\his}{\MB{\bar{h}}_{s}} % hidden top source
\newcommand{\hd}[1]{\MB{h}_{#1}} % hidden target 
\newcommand{\hb}[1]{\MB{\bar{h}}_{#1}} % hidden source
%\newcommand{\rnn}{\MB{T}_{d \times 2d}} % RNN
\newcommand{\rnn}{\MB{T}_{\text{rnn}}} % RNN
\newcommand{\lstm}{\MB{T}_{4d \times 2d}} % hidden top


%% Abi
\usepackage{amsthm} %for the proof environment
%theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}

% lstm
\newcommand{\ilt}{\MB{i^l_t}}
\newcommand{\flt}{\MB{f^l_t}}
\newcommand{\olt}{\MB{o^l_t}}
\newcommand{\glt}{\MB{\hat{h}^l_t}}
\newcommand{\hlt}{\MB{h^l_t}}
\newcommand{\clt}{\MB{c^l_t}}

\newcommand{\dilt}{\MB{\delta_i^l(t)}}
\newcommand{\dflt}{\MB{\delta_f^l(t)}}
\newcommand{\dolt}{\MB{\delta_o^l(t)}}
\newcommand{\dglt}{\MB{\delta_{\hat{h}}^l(t)}}
\newcommand{\dhlt}{\MB{\delta_h^l(t)}}
\newcommand{\dclt}{\MB{\delta_c^l(t)}}

\newcommand{\zilt}{z_i^l(t)}
\newcommand{\zflt}{z_f^l(t)}
\newcommand{\zolt}{z_o^l(t)}
\newcommand{\zglt}{z_{\hat{h}}^l(t)}
\newcommand{\zhlt}{z_h^l(t)}
\newcommand{\zclt}{z_c^l(t)}


% -- Document --
\begin{document}

% Title
\title{Neural Machine Translation}
\author{Minh-Thang Luong}
\principaladviser{Christopher D. Manning}
\firstreader{Dan Jurafsky}
\secondreader{Andrew Ng}
\thirdreader{Quoc V. Le}

% Preface
\beforepreface
%\input preface.tex
%\input ack.tex
\afterpreface


% -- Sections --
\chapter{Introduction}
\label{c:intro}
\input{1-intro.tex}

\chapter{Background}
\label{c:background}
\input{2-background.tex}

\chapter{Copy Mechanisms}
\label{c:copy}
\input{3-copy.tex}

\chapter{Attention Mechanisms}
\label{c:attention}
\input{4-attention.tex}

\chapter{Hybrid Models}
\label{c:hybrid}
\input{5-hybrid.tex}

\chapter{NMT Future}
\label{c:future}
\input{6-future.tex}

\chapter{Conclusion}
\label{c:conclude}
\input{7-conclude.tex}


% -- Appendix --
\appendix
\chapter{Miscellaneous}
\begin{lemma}
\label{l:diag_mul}
Let $\bm{u}$, $\bm{v}$ be any vectors and $\odot$ be element-wise vector multiplication, we have:
\begin{align}
\diag (\bm{u}) \cdot \bm{v} = \bm{u} \odot \bm{v}
\end{align}
\end{lemma}

\begin{lemma}
\label{l:chain_rule}
Let $l$ be a loss value that  in which we
already knew how to compute its gradient $\der{v}$ with respect to a vector $\bm{v}$. Given that
$\bm{v} = f(\bm{Wh})$, the gradients $\der{h}, \der{W}$ of the loss $l$ with respect to the vector
$\bm{h}$ and the matrix $\bm{W}$ can be derived as follows:
\begin{align}
\der{h} = \tp{\bm{W}} \cdot \paren{f'(\bm{Wh}) \odot \der{v}}
\\
\der{W} = \paren{f'(\bm{Wh}) \odot \der{v}} \cdot \tp{\bm{h}} 
\end{align}
\end{lemma}

\begin{proof}
Let $\bm{z} = \bm{Wh}$, we have the following derivations: %applying chain rules in vector calculus, 
\begin{align*}
\der{h} &= \fracder{\bm{z}}{\bm{h}} \cdot \fracder{\bm{v}}{\bm{z}}
\cdot \der{v} && \text{[Vector calculus chain rules]}\\
&= \fracder{\bm{Wh}}{\bm{h}} \cdot \fracder{f(\bm{z})}{\bm{z}}
\cdot \der{v} \\
&=\tp{\bm{W}} \cdot \diag \paren{f'(\bm{z})} \cdot \der{v} \\
&=\tp{\bm{W}} \cdot \paren{f'(\bm{Wh}) \odot \der{v}} &&
\text{[\lemmaref{l:diag_mul}]}
\end{align*}

Let $\tp{\bm{w}_i}$ be the i$^{\text{th}}$ row vector of matrix $\bm{W}$ and
$v_i, z_i$ be the i$^{\text{th}}$ elements of vectors $\bm{v}, \bm{z}$. Also
denoting $\der{w}_i, dv_i$ to be the gradients of $l$ with respect to $\bm{w}_i,
v_i$, we have:
\begin{align*}
d\bm{w}_i &= \fracder{z_i}{\bm{w}_i} \cdot \fracder{v_i}{z_i}
\cdot dv_i && \text{[Vector calculus chain rules]}\\
&= \fracder{\tp{\bm{w}_i}\bm{h}}{\bm{w}_i} \cdot f'(z_i) \cdot dv_i \\
&= \bm{h} \cdot f'(z_i) \cdot dv_i \\ 
d\tp{\bm{w}_i} &= \paren{f'(z_i) \cdot dv_i} \cdot
\tp{\bm{h}} && \text{[Tranposing]} \\
\der{W} &= \paren{f'(\bm{Wh}) \odot \der{v}} \cdot \tp{\bm{h}}
&& \text{[Concatenating row derivatives]}
\end{align*}
\end{proof}

\begin{corollary}
\label{c:chain_rule}
As a special case of \lemmaref{l:chain_rule}, when $f$ is an identity function,
i.e., $\bm{v} = \bm{Wh}$, we have:
\begin{align}
\der{h} = \tp{\bm{W}} \cdot \der{v}
\\
\der{W} = \der{v} \cdot \tp{\bm{h}} 
\end{align}
\end{corollary}

%\chapter{LSTM Backpropagation}
%\input{2-backwardprop}

\bibliographystyle{plainnat}
\bibliography{thesis}
\end{document}
