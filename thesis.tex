\documentclass[12pt]{report}
\usepackage{suthesis}
%\documentstyle[12pt,suthesis]{report}

% -- Imports --
% (general libraries)
\usepackage{times,latexsym,amsfonts,amssymb,amsmath,graphicx,bbm,rotating}
\usepackage{enumitem,multirow,hhline,stmaryrd,bussproofs,mathtools,siunitx,arydshln}
\usepackage{booktabs,xcolor,csquotes}
% (custom libraries)
\usepackage{fitch}
% (inline references)
\usepackage[square,sort,comma,numbers]{natbib}
% (tikz)
\usepackage{tikz}
\usetikzlibrary{shapes.arrows,chains,positioning,automata,trees,calc}
\usetikzlibrary{patterns}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
% (tikz dependency tree)
\usepackage{tikz-dependency,pifont}
% (print algorithms)
\usepackage[ruled,lined,linesnumbered]{algorithm2e}
% (custom)
\input std-macros.tex
\input macros.tex	
% (paper compilation hacks)
\def\newcite#1{\citet{#1}}
\definecolor{darkblue}{rgb}{0.0,0.0,0.4}

%% Thang
\usepackage{epigraph}
\renewcommand{\epigraphsize}{\small}
\setlength{\epigraphwidth}{0.8\textwidth}
\usepackage{pstricks} % for overlay latex on top of images

% to split on hyphens for long urls
\usepackage{url}
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
% the below works for pdflatex
%\usepackage[hyphens]{url} 
\usepackage{hyperref}


\definecolor{lightgreen}{HTML}{44B178}
\definecolor{lightblue}{HTML}{3B8CDA}
\newcommand{\error}[1]{{\color{red} #1}}
\newcommand{\ngram}{$n$-gram}
\newcommand{\word}[1]{``#1''}
\newcommand{\bfit}[1]{\textbf{\textit{#1}}}
\newcommand{\unk}{$<$\texttt{unk}$>$}
\newcommand{\sos}{$<$\texttt{sos}$>$}
\newcommand{\eos}{$<$\texttt{eos}$>$}
\newcommand{\eq}[1]{Eq.~(\ref{#1})}
\newcommand{\lemmaref}[1]{Lemma~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\nlmtext}{neural probabilistic language models}
\newcommand{\nlm}{NPLM}
\newcommand{\nlms}{NPLMs}
\newcommand{\secref}[1]{Section~(\ref{#1})}
\newcommand{\algo}[1]{Algorithm~\ref{#1}}

% math operators
%\DeclareMathOperator*{\argmax}{\arg\,\max}
%\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\e}{e}
\DeclareMathOperator{\trace}{Tr}
\DeclareMathOperator{\sigm}{sigm}
\DeclareMathOperator{\sigmoid}{sigmoid}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\Index}{Index}

% math
\newcommand{\MB}[1]{\mbox{\boldmath{$#1$}}} % boldface for matrix and vectors
% derivative fractions
\newcommand{\parfrac}[1]{\frac{\partial}{\partial #1}} 
% fraction derivatives
\newcommand{\fracder}[2]{\frac{\partial #1}{\partial #2}}
% sum and product
\newcommand{\sumkn}{\sum_{k=1}^{n}} %I use this when deriving backprop eqns. n is the LSTM block size and k is a better choice than i because it won't get confused with the input gate
% plus equal
\newcommand{\pluseq}{\mathrel{+}=}
% transpose
\newcommand{\tp}[1]{#1^\top}
% round bracket
\newcommand{\open}[1]{\left(#1\right)}
\newcommand{\paren}[1]{\left(#1\right)}
% norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% grad
\newcommand{\grad}[1]{\nabla #1}

%%% math commands %%%
% variables 
\usepackage{bm}
\newcommand{\x}[1]{\bm{x}_{#1}}
\newcommand{\y}[1]{\bm{y}_{#1}}
\newcommand{\z}[1]{\bm{z}_{#1}}
\newcommand{\s}[1]{\bm{s}_{#1}}
\newcommand{\prob}[1]{\bm{p}_{#1}}
\newcommand{\der}[1]{d\bm{#1}}
\newcommand{\W}[1]{\bm{W}_{#1}}
\newcommand{\inR}[1]{\in \mathbb{R}^{#1}}
\newcommand{\yrange}[2]{y_{\overline{#1,#2}}}
\newcommand{\ytop}[1]{y^{(#1)}}
\newcommand{\thetav}{\bm{\theta}}

\newcommand{\edot}{\circ}
% lstm
\newcommand{\mem}[1]{\bm{c}_{#1}}
\newcommand{\ig}{\bm{i}_{t}}
\newcommand{\fg}{\bm{f}_{t}}
\newcommand{\og}{\bm{o}_{t}}
\newcommand{\hg}{\MB{\hat{h}_t}}
%\newcommand{\iparam}{\MB{T}_{\text{i}}} % input params 
%\newcommand{\fparam}{\MB{T}_{\text{f}}} % forget params 
%\newcommand{\oparam}{\MB{T}_{\text{o}}} % output params 
\newcommand{\hparam}{\MB{T}_{\text{h}}} % hidden params 
\newcommand{\xparam}{\MB{T}_{\text{x}}} % hidden params 
\newcommand{\ut}{\bm{u}_t}

% NMT
\newcommand{\tgt}[1]{y_{#1}} % target word
\newcommand{\src}[1]{x_{#1}} % source word
\newcommand{\al}{\MB{a}_{t}} % align
\newcommand{\hid}[1]{\bm{h}_{#1}}
\newcommand{\his}{\MB{\bar{h}}_{s}} % hidden top source
\newcommand{\hs}{\MB{\tilde{h}}_{t}} %h softmax
\newcommand{\hd}[1]{\MB{h}_{#1}} % hidden target 
\newcommand{\hb}[1]{\MB{\bar{h}}_{#1}} % hidden source
%\newcommand{\rnn}{\MB{T}_{d \times 2d}} % RNN
\newcommand{\rnn}{\MB{T}_{\text{rnn}}} % RNN
% \newcommand{\lstm}{\MB{T}_{4d \times 2d}} % lstm 
\newcommand{\lstm}{\MB{T}_{\text{lstm}}} % LSTM 
\newcommand{\hi}{\MB{h}_{t}} % hidden top
\newcommand{\co}{\MB{c}_{t}} % context


%% Abi
\usepackage{amsthm} %for the proof environment
%theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}

%% Chapter 3
\newcommand{\unkword}[1]{{\bf {\it {\underline{#1}}}}}
\newcommand{\unksym}{{\it \texttt{\underline{unk}}}}
\newcommand{\eossym}{$<$\texttt{eos}$>$}
\newcommand{\unkcopy}[1]{{\bf {\it {\texttt{\underline{unk}$_{#1}$ }}}}}
\newcommand{\unknull}{{\bf {\it {\texttt{\underline{unk}$_{\emptyset}$}}}}}
\newcommand{\unktext}[1]{{\bf {\it \texttt{\underline{unk}}$_{#1}$}}}
\newcommand{\pos}[1]{{\bf {\it {\texttt{p$_{#1}$}}}}}
\newcommand{\posnull}{{\bf {\it {\texttt{p$_{\emptyset}$}}}}}
\newcommand{\postext}[1]{{\bf {\it \texttt{p}$_{#1}$}}}
\newcommand{\unkpos}[1]{{\bf {\it {\texttt{\underline{unkpos}$_{#1}$}}}}}
\newcommand{\unkpostext}[1]{{\bf {\it \texttt{unkpos}$_{#1}$}}}

% experiment results
\newcommand{\bestbleu}{35.6}
\newcommand{\bestbleuunk}{37.5} % after unk translation
\newcommand{\bestbleuunkwmt}{36.6} % after unk translation
\newcommand{\bestunkimp}{2.8} % best of (bleuunk - bleu)
\newcommand{\unkimp}{1.9} % bestbleuunk - bestbleu
\newcommand{\unkimpilya}{2.7} % compare with ilya best system 34.8
\newcommand{\imprare}{4.8} % compare the last group of 500 sentences with the most rare words before and after unk translations (in Rarity Analysis)

%% Chapter 4
\newcommand{\correct}[1]{\textit{\color{blue} #1}}
\newcommand{\wrong}[1]{\textbf{\color{red} #1}}
\newcommand{\pt}{p_{t}} % context
\newcommand{\sotaold}{23.0} % WMT'14 English-German
\newcommand{\sotanew}{25.9} % WMT'15 English-German
\newcommand{\attngain}{5.0} % max improvement when using attention 
\newcommand{\localm}{local-m} % max improvement when using attention 
\newcommand{\localp}{local-p} % max improvement when using attention 
\DeclareMathOperator{\alignf}{align}
\DeclareMathOperator{\score}{score}

%% Chapter 5
\newcommand{\hide}[1]{}
\newcommand{\source}[1]{\bi{#1}}
\newcommand{\close}[1]{\underline{\color{brown} #1}}
% char
\newcommand{\Ww}{\MB{W}} % W_h 
\newcommand{\Wc}{\MB{\breve{W}}} % W_c
\newcommand{\hc}{\MB{\breve{h}}_{t}} %h softmax char

\newcommand{\hybrid}{hybrid-NMT}
\newcommand{\rare}{$<${\it rare}$>$}

% best models
\newcommand{\modelword}{{\it (d)}}
\newcommand{\modelchar}{{\it (g)}}
\newcommand{\modelsmall}{{\it (k)}}
\newcommand{\model}{{\it (l)}}
\newcommand{\ensbleu}{20.7}
\newcommand{\gain}{2.1{-}11.4}
\newcommand{\chr}{chrF$_3$}




% -- Document --
\begin{document}

% Title
\title{Neural Machine Translation}
\author{Minh-Thang Luong}
\principaladviser{Christopher D. Manning}
\firstreader{Dan Jurafsky}
\secondreader{Andrew Ng}
\thirdreader{Quoc V. Le}

% Preface
\beforepreface
%\input preface.tex
%\input ack.tex
\afterpreface


% -- Sections --
\chapter{Introduction}
\label{c:intro}
\input{1-intro.tex}

\chapter{Background}
\label{c:background}
\input{2-background.tex}

\chapter{Copy Mechanisms}
\label{c:copy}
\input{3-copy.tex}

\chapter{Attention Mechanisms}
\label{c:attention}
\input{4-attention.tex}

\chapter{Hybrid Models}
\label{c:hybrid}
\input{5-hybrid.tex}

\chapter{NMT Future}
\label{c:future}
\input{6-future.tex}

\chapter{Conclusion}
\label{c:conclude}
\input{7-conclude.tex}


% -- Appendix --
\appendix
\chapter{Miscellaneous}
\label{c:misc}
\begin{lemma}
\label{l:diag_mul}
Let $\bm{u}$, $\bm{v}$ be any vectors and $\edot$ be element-wise vector multiplication, we have:
\begin{align}
\diag (\bm{u}) \cdot \bm{v} = \bm{u} \edot \bm{v}
\end{align}
\end{lemma}

\begin{lemma}
\label{l:chain_rule}
Let $l$ be a loss value that  in which we
already knew how to compute its gradient $\der{v}$ with respect to a vector $\bm{v}$. Given that
$\bm{v} = f(\bm{Wh})$, the gradients $\der{h}, \der{W}$ of the loss $l$ with respect to the vector
$\bm{h}$ and the matrix $\bm{W}$ can be derived as follows:
\begin{align}
\der{h} = \tp{\bm{W}} \cdot \paren{f'(\bm{Wh}) \edot \der{v}}
\\
\der{W} = \paren{f'(\bm{Wh}) \edot \der{v}} \cdot \tp{\bm{h}} 
\end{align}
\end{lemma}

\begin{proof}
Let $\bm{z} = \bm{Wh}$, we have the following derivations: %applying chain rules in vector calculus, 
\begin{align*}
\der{h} &= \fracder{\bm{z}}{\bm{h}} \cdot \fracder{\bm{v}}{\bm{z}}
\cdot \der{v} && \text{[Vector calculus chain rules]}\\
&= \fracder{\bm{Wh}}{\bm{h}} \cdot \fracder{f(\bm{z})}{\bm{z}}
\cdot \der{v} \\
&=\tp{\bm{W}} \cdot \diag \paren{f'(\bm{z})} \cdot \der{v} \\
&=\tp{\bm{W}} \cdot \paren{f'(\bm{Wh}) \edot \der{v}} &&
\text{[\lemmaref{l:diag_mul}]}
\end{align*}

Let $\tp{\bm{w}_i}$ be the i$^{\text{th}}$ row vector of matrix $\bm{W}$ and
$v_i, z_i$ be the i$^{\text{th}}$ elements of vectors $\bm{v}, \bm{z}$. Also
denoting $\der{w}_i, dv_i$ to be the gradients of $l$ with respect to $\bm{w}_i,
v_i$, we have:
\begin{align*}
d\bm{w}_i &= \fracder{z_i}{\bm{w}_i} \cdot \fracder{v_i}{z_i}
\cdot dv_i && \text{[Vector calculus chain rules]}\\
&= \fracder{\tp{\bm{w}_i}\bm{h}}{\bm{w}_i} \cdot f'(z_i) \cdot dv_i \\
&= \bm{h} \cdot f'(z_i) \cdot dv_i \\ 
d\tp{\bm{w}_i} &= \paren{f'(z_i) \cdot dv_i} \cdot
\tp{\bm{h}} && \text{[Tranposing]} \\
\der{W} &= \paren{f'(\bm{Wh}) \edot \der{v}} \cdot \tp{\bm{h}}
&& \text{[Concatenating row derivatives]}
\end{align*}
\end{proof}

\begin{corollary}
\label{c:chain_rule}
As a special case of \lemmaref{l:chain_rule}, when $f$ is an identity function,
i.e., $\bm{v} = \bm{Wh}$, we have:
\begin{align}
\der{h} = \tp{\bm{W}} \cdot \der{v}
\\
\der{W} = \der{v} \cdot \tp{\bm{h}} 
\end{align}
\end{corollary}

\begin{lemma}
\label{l:edot_der}
Let $\bm{u}$, $\bm{v}, \bm{s}$ be any vectors such that  $\bm{s} = \bm{u} \edot
f(\bm{v})$. Also, let $d\bm{u}$, $d\bm{v}, d\bm{s}$ be the gradients of a loss $l$ with
respect to the corresponding vectors. We have:
\begin{align}
d\bm{u} = f(\bm{v}) \edot d\bm{s} \\
d\bm{v} = f'(\bm{v}) \edot \bm{u} \edot d\bm{s} 
%d\bm{u} = \diag \paren{f(\bm{v})} \cdot d\bm{s} \\
%d\bm{v} = \diag \paren{f'(\bm{v}} \edot \bm{u}} \cdot d\bm{s} 
\end{align}
\end{lemma}

%\begin{proof}
%\begin{align}
%d\bm{u} &= \parder{\diag \paren{f(\bm{v})} \cdot d\bm{s} \\
%\end{align}
%\end{proof}

\begin{corollary}
\label{c:edot_der}
As a special case of \lemmaref{l:edot_der} when $f$ is an identity function,
i.e.,  $\bm{s} = \bm{u} \edot \bm{v}$. We have:
\begin{align}
%d\bm{u} = \diag (\bm{v}) \cdot d\bm{s} \\
%d\bm{v} = \diag (\bm{u}) \cdot d\bm{s} 
d\bm{u} = \bm{v} \edot d\bm{s} \\
d\bm{v} = \bm{u} \edot d\bm{s} 
\end{align}
\end{corollary}


%\chapter{LSTM Backpropagation}
%\input{2-backwardprop}

\bibliographystyle{plainnat}
\bibliography{thesis}
\end{document}
