\prefacesection{Abstract}
Being able to communicate seamlessly across the entire repertoire of human
languages is, to me, an ultimately rewarding goal for an intelligent system.
Despite great progress in the field of Statistical Machine Translation (SMT)
over the past two decades, translation quality has not yet satisfied users;
at the same time, SMT systems have become increasing complex with many different
components built separately, rendering it extremely difficult to make further
advancement. Recently, Neural Machine Translation (NMT) emerges as a promising
solution to the problem of machine translation. At its core, NMT consists of a
single deep neural network with millions of neurons that learn to directly map
source sentences to target sentences. NMT is powerful because it is an
end-to-end deep-learning framework that is significantly better than SMT in
capturing long-range dependencies in sentences and generalizing well to unseen
texts.

This dissertation presents all of the essence of Neural Machine Translation
(NMT), through which I discuss how I have pushed the limits of NMT, making it
applicable to a wide variety of languages with state-of-the-art performance. My
contributions include addressing the rare word problem with copy mechanisms,
improving the attention mechanism to better select local contexts in the source
sentence, and translating at the character level with a hybrid architecture.
Towards the future of NMT, I discuss how to utilize data from a wide variety of
tasks such as parsing, image caption generation, and unsupervised learning to
improve translation; as well as how to compress NMT models for mobile devices. I
conclude with how my work influences subsequent research as well as provide an
in-depth coverage on the existing research landscape, highlight potential
research directions, and speculate on future elements needed to further advance
NMT.
