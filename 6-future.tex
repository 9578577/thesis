In previous chapters, my effort to improving neural machine
translation has centered around enhancing the model architecture to address
different needs such as translating long sentences or coping with
complex vocabularies. In this chapter, I switch gear to examine more ``external''
aspects of NMT, which is also a way for me to take a quick peek into the future of NMT.
Specifically, I first examine in Section~\ref{sec:multi-task} how NMT can be improved by utilizing data from
not only the translation but also other tasks such as parsing, image caption
generation, and unsupervised learning. This is framed under the {\it multi-task}
setting which I believe is important for NMT future given a humongous amount of
data available in the world growing at an exponentially fast pace. The second
aspect that I examine is on making NMT models smaller, a topic of 
increasing importance as mobile devices become dominant. Specifically, in
Section~\ref{sec:nmt-compression}, I cast such
desiderata as a {\it model compression} problem in which I answer how much we can
reduce the sizes of NMT models without sacrifice in performance and reveal
interesting patterns in the parameter space of NMT. Lastly, in
Section~\ref{sec:outlook}, I highlight other future trends and potential
research directions for NMT.


\section{Multi-task Sequence to Sequence Learning}
\label{sec:multi-task}
\input{6-1_intro}

\subsection{Multi-task Sequence-to-Sequence Learning}
\label{subsec:multi}
\input{6-1_multi}

\subsection{Experiments}
\label{sec:6_1_exp}
\input{6-1_exp}

\subsection{Conclusion}
\label{sec:conclude}
\input{6-1_conclusion}

\section{Compression of NMT Models via Pruning}
\label{sec:nmt-compression}
\input{6-2_intro}

\subsection{Related Work}
\label{subsec:related}
\input{6-2_related}


\subsection{Our Approach}
\label{subsec:approach}
\input{6-2_approach}

\subsection{Experiments}
\label{subsec:exp}
\input{6-2_exp}

\subsection{Conclusion}
\label{subsec:conclusion}
\input{6-2_conclusion}

\section{Future Outlook}
\label{sec:outlook}
In this section, I will take the liberty to speculate on the future of NMT by first extending on ideas I have just discussed, multi-task learning and model compression. After that, I will talk about two other future trends on handling long context and new approaches to training sequence models beside maximum-likelihood estimation.\footnote{Some of the content of this section is based on the NMT tutorial that I, Kyunghuyn Cho, and Christopher D. Manning gave at ACL 2016 \url{https://sites.google.com/site/acl16nmt/}.}

\subsection{Multi-task and Semi/Un-supervised Learning}
In \secref{sec:multi-task}, I have assessed the feasibility of utilizing other tasks, such as parsing, image caption, and unsupervised learning, to improve translation. The positive gains in the translation quality that we achieved further reinforces my belief that multi-task learning is an important direction for the future of NMT (and even for Artificial General Intelligence). In the short-term future, as successors to our work, there have been fruitful results in building {\it multilingual} NMT systems \cite{zoph16,firat16,gnmt16multi,ha16} in which translations in multiple languages are viewed as different tasks. A nice by-product of such a system is the ability to do {\it zero-shot learning} which has been demonstrated convincingly by \newcite{gnmt16multi}. In that work, the authors built a single model that can do translation for 12 language pairs using the same sub-word vocabulary. Even more exciting, they can translate reasonably well for unseen language pairs at training time without using a pivot language. Ultimately, as what human does, it will be tremendously powerful if we can successfully learn from the data of all (sequence-to-sequence) tasks and construct a single model that can accomplish multiple goals, such as speech recognition and translation, at the same time. In this way, an intelligent system can take speech, for example in English, as input and produces on the fly a text translation, say in Urdu or Vietnamese, even though it has never seen any training data between the speech and text of that language pair.

Semi-supervised learning will also play a crucial role in the future of NMT systems. When mentioning about semi-supervised learning, I also imply the importance of unsupervised learning: any successful unsupervised learning model in text should provide a general form of language understanding that will be benefical to downstream tasks that require supervision, e.g., \cite{dai15}.
In \secref{sec:6_1_exp}, I have also shown preliminary performance gains in translation by having auto-encoders or skip-thought training as unsupervised tasks in a multi-task setting. Such model, however, can only utilize a small amount of monolingual text, the data that exists in vast quantity. Human, in contrast, has the ability to learn a new language by first having some form of supervision such as a language teacher or a grammar book; afterwards, they can simply read books or material in that foreign language and keep improving their translation capabilities. Future NMT systems should be able to do so. In fact, recent approaches in {\it dual translation} models \cite{sennrich16mono,xia16}, which involve two back-and-forth translation models between a language pair, are heading towards that direction. In the former work, the authors simply use the reverse translation model to generate more parallel training data from the target-language monolingual text, which helps alleviating over-fitting. The latter work is closer to what I envision for the future: starting with 10\% of the bilingual data, the authors train both source-to-target and target-to-source models; then, through a Reinforcement Learning setting on monolingual data only, the two models help each other in improving their translation abilities. Using this approach, the dual-translation system can achieve comparable performance to NMT models trained on the full bilingual data. Ideally, we would want to keep learning from monolingual data forever and getting better and better over time.

%,cheng16
\subsection{Model Compression and Knowledge Distillation}
As we have discussed, the need for {\it model compression} is inevitable as mobile devices become ubiquitous nowsadays and we want to make NMT models small enough to fit onto the device. In \secref{sec:nmt-compression}, I found it rather surprising that the parameters of NMT models can be pruned up to 80\% without any loss in performance as long as I retrain the pruned models. What I did, however, was only a proof-of-concept to show that there is a great redundancy in the parameter space of NMT models and it can be made smaller to fit onto mobile devices. I believe the future for NMT (and deep learning models in general) will involve about dealing efficiently with low-precision arithmetics \cite{courbariaux14,gupta15} and sparse models.

In parallel, the idea of {\it knowledge distillation} \cite{hinton15}, also proves to be of great importance in deep learning. What happens in practice is often, one can improve the system performance (sometimes by quite a lot) simply by training multiple models, an ensemble, and then averaging the predictions. Such a process is quite tedious and computationally expensive to deploy to users. The idea of distillation arises to address this problem by building a single neural network that can mimic the behavior of an ensemble. This turns out to work very well for NMT as demonstrated by \newcite{kim16distill}. Instead of trying to mimic an ensemble of models, they try build a smaller ({\it student}) network to learn from a larger ({\it teacher}) network. This not only speeds up inference time but also achieves the goal of making the models smaller. Additionally, they applied the absolute-value pruning technique that I proposed for NMT to achieve further model compression, which is quite remarkable. I am looking forward to see knowledge distillation applied to more than just one and over a variety of NMT models.


\subsection{Larger Context}
\subsection{Beyond Maximum Likelihood Estimation}
\cite{xia16,tu16}
