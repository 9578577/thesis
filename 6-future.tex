In previous chapters, my effort to improving neural machine
translation has centered around enhancing the model architecture to address
different needs such as translating long sentences or coping with
complex vocabularies. In this chapter, I switch gear to examine more ``external''
aspects of NMT, which is also a way for me to take a quick peek into the future of NMT.
Specifically, I first examine in Section~\ref{sec:multi-task} how NMT can be improved by utilizing data from
not only the translation but also other tasks such as parsing, image caption
generation, and unsupervised learning. This is framed under the {\it multi-task}
setting which I believe is important for NMT future given a humongous amount of
data available in the world growing at an exponentially fast pace. The second
aspect that I examine is on making NMT models smaller, a topic of 
increasing importance as mobile devices become dominant. Specifically, in
Section~\ref{sec:nmt-compression}, I cast such
desiderata as a {\it model compression} problem in which I answer how much we can
reduce the sizes of NMT models without sacrifice in performance and reveal
interesting patterns in the parameter space of NMT. Lastly, in
Section~\ref{sec:outlook}, I highlight other future trends and potential
research directions for NMT.


\section{Multi-task Sequence to Sequence Learning}
\label{sec:multi-task}
\input{6-1_intro}

\subsection{Multi-task Sequence-to-Sequence Learning}
\label{subsec:multi}
\input{6-1_multi}

\subsection{Experiments}
\label{sec:6_1_exp}
\input{6-1_exp}

\subsection{Conclusion}
\label{sec:conclude}
\input{6-1_conclusion}

\section{Compression of NMT Models via Pruning}
\label{sec:nmt-compression}
\input{6-2_intro}

\subsection{Related Work}
\label{subsec:related}
\input{6-2_related}


\subsection{Our Approach}
\label{subsec:approach}
\input{6-2_approach}

\subsection{Experiments}
\label{subsec:exp}
\input{6-2_exp}

\subsection{Conclusion}
\label{subsec:conclusion}
\input{6-2_conclusion}

\section{Future Outlook}
\label{sec:outlook}
In this section, I will take the liberty to speculate on the future of NMT by first extending on ideas I have just discussed, multi-task learning and model compression. After that, I will talk about two other future trends on training sequence models beside maximum-likelihood estimation as well as maintaining coherence and style in translation.\footnote{Some of the content of this section is based on the NMT tutorial that I, Kyunghuyn Cho, and Christopher D. Manning gave at ACL 2016 \url{https://sites.google.com/site/acl16nmt/}.}

\subsection{Multi-task and Semi/Un-supervised Learning}
In \secref{sec:multi-task}, I have assessed the feasibility of utilizing other tasks, such as parsing, image caption, and unsupervised learning, to improve translation. The positive gains in the translation quality that we achieved further reinforces my belief that multi-task learning is an important direction for the future of NMT (and even for Artificial General Intelligence). In the short-term future, as successors to our work, there have been fruitful results in building {\it multilingual} NMT systems \cite{zoph16,firat16,gnmt16multi,ha16} in which translations in multiple languages are viewed as different tasks. A nice by-product of such a system is the ability to do {\it zero-shot learning} which has been demonstrated convincingly by \newcite{gnmt16multi}. In that work, the authors built a single model that can do translation for 12 language pairs using the same sub-word vocabulary. Even more exciting, they can translate reasonably well for unseen language pairs at training time without using a pivot language. Ultimately, as what human does, it will be tremendously powerful if we can successfully learn from the data of all (sequence-to-sequence) tasks and construct a single model that can accomplish multiple goals, such as speech recognition and translation, at the same time. In this way, an intelligent system can take speech, for example in English, as input and produces on the fly a text translation, say in Urdu or Vietnamese, even though it has never seen any training data between the speech and text of that language pair.

{\it Semi-supervised} learning will also play a crucial role in the future of NMT systems. When mentioning about semi-supervised learning, I also imply the importance of unsupervised learning: any successful unsupervised learning model in text should provide a general form of language understanding that will be beneficial to downstream tasks that require supervision, e.g., \cite{dai15}.
In \secref{sec:6_1_exp}, I have also shown preliminary performance gains in translation by having auto-encoders or skip-thought training as unsupervised tasks in a multi-task setting. Such model, however, can only utilize a small amount of monolingual text, the data that exists in vast quantity. Human, in contrast, has the ability to learn a new language by first having some form of supervision such as a language teacher or a grammar book; afterwards, they can simply read books or material in that foreign language and keep improving their translation capabilities. Future NMT systems should be able to do so. 

In fact, recent approaches in {\it dual translation} models \cite{sennrich16mono,xia16}, which involve two back-and-forth translation models between a language pair, are heading towards that direction. In the former work, the authors simply use the reverse translation model to generate more parallel training data from the target-language monolingual text, which helps alleviating over-fitting. The latter work is closer to what I envision for the future: starting with 10\% of the bilingual data, the authors train both source-to-target and target-to-source models; then, through a Reinforcement Learning setting on monolingual data only, the two models help each other in improving their translation abilities. Using this approach, the dual-translation system can achieve comparable performance to NMT models trained on the full bilingual data. However, the approach is not yet scaled well to utilize the full monolingual data. Ideally, we would want to keep learning from monolingual data forever and getting better and better over time.

%,cheng16
\subsection{Model Compression and Knowledge Distillation}
As we have discussed, the need for {\it model compression} is inevitable as mobile devices become ubiquitous nowadays and we want to make NMT models small enough to fit onto the device. In \secref{sec:nmt-compression}, I found it rather surprising that the parameters of NMT models can be pruned up to 80\% without any loss in performance as long as I retrain the pruned models. What I did, however, was only a proof-of-concept to show that there is a great redundancy in the parameter space of NMT models and it can be made smaller to fit onto mobile devices. I believe the future for NMT (and deep learning models in general) will involve about dealing efficiently with low-precision arithmetics \cite{courbariaux14,gupta15} and sparse models.

In parallel, the idea of {\it knowledge distillation} \cite{hinton15}, also proves to be of great importance in deep learning. What happens in practice is often, one can improve the system performance (sometimes by quite a lot) simply by training multiple models, an ensemble, and then averaging the predictions. Such a process is quite tedious and computationally expensive to deploy to users. The idea of distillation arises to address this problem by building a single neural network that can mimic the behavior of an ensemble. This turns out to work very well for NMT as demonstrated by \newcite{kim16distill}. Instead of trying to mimic an ensemble of models, they try build a smaller ({\it student}) network to learn from a larger ({\it teacher}) network. This not only speeds up inference time but also achieves the goal of making NMT models smaller. Additionally, they applied the absolute-value pruning technique that I proposed in \secref{sec:nmt-compression} to achieve further model compression for NMT, which is quite remarkable. I am looking forward to see knowledge distillation applied to not just one but over a variety of NMT models.

\subsection{Beyond Maximum Likelihood Estimation}
So far, the standard maximum likelihood estimation (MLE) approach 
to optimizing the conditional probability of a target sentence given a source sentence 
has served us well in training NMT models. However, as NMT has reached a new milestone of completely surpassing phrase-based models and being used in commercial systems \cite{gnmt16,systran16}, it is time to look beyond MLE to further advance NMT. Researchers have previously and recently started to identifying major problems of using MLE to train sequence models. The first one is the {\it exposure bias} problem \cite{bengio15} arose due to mismatch between training and inference: at training time, correct words from the data distribution, are always provided; where as at inference time, the most likely words predicted by the model are used as input to the next time step. The second one is the {\it loss-evaluation mismatch} problem \cite{ranzato16} due to the fact that we train models with word-level loss, e.g., the cross-entropy objective, but evaluate the final performance using sequence-level discrete metrics such as BLEU \cite{Papineni02bleu}. 

This is a research direction that I find extremely fascinating as evidenced by a diverse set of recent work trying to address the aforementioned problems. Here, I try to  highlight some of the work though readers will notice that the general ideas revolve around incorporating inference into training and maximizing the sequence-wise global loss. For example, \newcite{bengio15} address the exposure bias problem using a {\it scheduled sampling} approach that bridges the gap between training and inference by alternating between using the correct words as input and words predicted by the model during training; the optimization procedure remains to be MLE. \newcite{ranzato16} incorporates sequence-level metrics, such as BLEU for translation and ROUGE for summarization, through the {\it reinforcement learning} (RL) framework, specifically using the REINFORCE, or {\it policy gradient}, algorithm \cite{reinforce}. Since RL requires drawing samples from the model distribution, this approach does address the exposure bias problem as well. There is, however, a challenge in applying RL to languages, that is, the action space, or the vocabulary, is too large. As such, to speed up learning, the authors of \cite{ranzato16} propose an approach, named MIXER, that combines both MLE and RL training: MLE is used for pretraining the network initially as well as to help RL produce more stable sequences. Alternatively, \newcite{bahdanau16actor} use the {\it actor-critic} approach to find better actions, i.e., words given a context, which leads to faster convergence and better final performance.

There are also many related approaches for sequence-level training. For example, 
\newcite{shen16} employ the {\it minimum risk training} framework to minimize the expected (non-differentiable) loss on the training data, which happens to be the same as the policy gradient loss. However, there are differences in how candidates are sampled and how the expected loss is approximated using a renormalized distribution over the candidates only. 
\newcite{norouzi16} offer insights on how MLE and RL objectives are related as well as propose a hybrid approach between the two, namely {\it 
reward-augmented MLE}, that is computationally efficient and avoids the aforementioned ``tricks'', such as pretraining, actor-critic, and variance reduction, to make RL work.
While all of the above work incorporate stochastic inference to training, \newcite{wiseman16} consider adding deterministic inference to training through {\it beam-search optimization}.
The authors utilize the max-margin framework and substitute the RNN locally-normalized scores with the partial sentence-level BLEU scores. This approach has several advantages that it preserves the proven model architecture of seq2seq and at the same time addresses
the well-known {\it label-bias} problem \cite{lafferty01} arose whenever the locally-normalized scores from RNNs are used.
%which was recently brought up by \newcite{wiseman16} in the context of RNNs: we build locally normalized word-level distributions at each time step, the product of which does not reflect the true sequence-level distribution. 
%need beam-search to find margin violations

As I mentioned, this is an exciting research area with many different approaches. While the list does not simply stop there and it remains to be seen which approaches will stand the test of time, I think an important big picture here is that we are coming closer to optimize arbitrary objectives. Among the different choices, I hope {\it coherence} and {\it style}, which I will talk next, will be featured some day.

%\cite{bengio15}
%As we move forward, researchers have started looking for ways to enhancing MLE or replacing it. What are the problems with MLE?
%
%
%search for larger beam
%
%exposure bias problem
%mismatch between train and test
%
%strong correlation between perplexity and BLEU scores \secref{subsec:effects}. however, we might want to optimize for other metrics such as coherence and style, fluency, adequacy
%
%stochastic inference
%
%policy gradient
%\cite{ranzato16}
%\cite{xia16}
%
%
%deterministic inference
%\cite{wiseman16}
%
%
%\cite{tu16}


\subsection{Translation with Coherence and Style}
Up until now, translation only happens at the {\it sentence} level: a paragraph or a document is split into multiple sentences, each of which is translated in isolation. This is, unfortunately, neither how human translates nor the way the meaning of texts is derived. Behind a sequence of sentences, there is often a high-level, sometimes complex, organization of thoughts, the {\it discourse structure} \cite{mann1988} which captures relationships among different text units such as comparison, elaboration, and evidence. Professional translators do not translate, for example a 4-sentence paragraph, using the exact number of sentences in the source; they can use more or less sentences depending on their understanding of the source text and how thoughts are presented in the target language. Early work \cite{marcu2000} hinted that modeling discourse structure is useful for machine translation, especially for distant language pairs such as English and Japanese.

The big picture here, in my opinion, is that future NMT systems should handle {\it coherence} and {\it style}, the two fundamental elements present in professional translations, on which current machine translation systems are missing. Coherence means translation beyond the sentence level and to achieve that, one might need to consider linguistic insights such as discourse structure analysis and coreference solution, just like how the attention model in NMT was motivated by word alignment notion in machine translation. 
Architecture-wise, I think models that can handle well very long sequences such as {\it hierarchical models} will be useful in maintaining translation coherence. In this paper \cite{li15}, I and other colleagues demonstrated the effectiveness of hierarchical models in constructing representations for long-text sequences such as paragraphs and documents in an autoencoder setting.
% Specifically, we first connect words to build up representations for sentences, then we connect sentences together to derive paragraph representations. 
Besides, hierarchical models have also proven its usefulness in other areas such as speech recognition \cite{chan16} and dialogue systems \cite{serban16}.

Maintaining style is an even harder problem. Not only does the translation system need to ensure coherence but it also has to recognize and preserve the tone of the source text, e.g., whether this is a formal text in business setting or if the text is informal and has some sense of humor, satire, etc. Accomplishing this will require deep understanding of languages such as recognizing idiomatic phrases, scare quotes, slang usages, and even implicit cultural referents. I believe insights from the area of dialogue systems will benefit style translation as we start seeing work in adding personalization to conversation dialogues \cite{li16,alrfou16}.

% work \cite{li16,alrfou16} has already begun in adding personalization to conversational dialogues.

Lastly, to make our progress measurable, evaluation datasets and proper automatic metrics will be tremendously useful as how BLEU \cite{Papineni02bleu} has helped advanced the field of machine translation. The authors in \cite{smith15} have put up a proposal for a coherence corpus in machine translation and I am looking forward to many more of such resources in the future.

% effective attention mechanism: hierarchical structures \cite{chan16}
% progress will not be made without a proper way of evaluation.

