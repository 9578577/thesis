In previous chapters, my effort to improving neural machine
translation has centered around enhancing the model architecture to address
different needs such as translating long sentences or coping with
complex vocabularies. In this chapter, I switch gear to examine more ``external''
aspects of NMT, which is also a way for me to take a quick peek into the future of NMT.
Specifically, I first examine in Section~\ref{sec:multi-task} how NMT can be improved by utilizing data from
not only the translation but also other tasks such as parsing, image caption
generation, and unsupervised learning. This is framed under the {\it multi-task}
setting which I believe is important for NMT future given a humongous amount of
data available in the world growing at an exponentially fast pace. The second
aspect that I examine is on making NMT models smaller, a topic of 
increasing importance as mobile devices become dominant. Specifically, in
Section~\ref{sec:nmt-compression}, I cast such
desiderata as a {\it model compression} problem in which I answer how much we can
reduce the sizes of NMT models without sacrifice in performance and reveal
interesting patterns in the parameter space of NMT. Lastly, in
Section~\ref{sec:outlook}, I highlight other future trends and potential
research directions for NMT.


\section{Multi-task Sequence to Sequence Learning}
\label{sec:multi-task}
\input{6-1_intro}

\subsection{Multi-task Sequence-to-Sequence Learning}
\label{subsec:multi}
\input{6-1_multi}

\subsection{Experiments}
\label{sec:6_1_exp}
\input{6-1_exp}

\subsection{Conclusion}
\label{sec:conclude}
\input{6-1_conclusion}

\section{Compression of NMT Models via Pruning}
\label{sec:nmt-compression}
\input{6-2_intro}

\subsection{Related Work}
\label{subsec:related}
\input{6-2_related}


\subsection{Our Approach}
\label{subsec:approach}
\input{6-2_approach}

\subsection{Experiments}
\label{subsec:exp}
\input{6-2_exp}

\subsection{Conclusion}
\label{subsec:conclusion}
\input{6-2_conclusion}

\section{Future Outlook}
\label{sec:outlook}
In this section, I will take the liberty to speculate on the future of NMT by first extending on ideas I have just discussed, multi-task learning and model compression. After that, I will talk about two other future trends on handling long context and new approaches to training sequence models beside maximum-likelihood estimation.\footnote{Some of the content of this section is based on the NMT tutorial that I, Kyunghuyn Cho, and Christopher D. Manning gave at ACL 2016 \url{https://sites.google.com/site/acl16nmt/}.}

\subsection{Multi-task and Semi/Un-supervised Learning}
In \secref{sec:multi-task}, I have assessed the feasibility of utilizing other tasks, such as parsing, image caption, and unsupervised learning, to improve translation. The positive gains in the translation quality that we achieved further reinforces my belief that multi-task learning is an important direction for the future of NMT (and even for Artificial General Intelligence). In the short-term future, as successors to our work, there have been fruitful results in building {\it multilingual} NMT systems \cite{zoph16,firat16,gnmt16multi,ha16} in which translations in multiple languages are viewed as different tasks. A nice by-product of such a system is the ability to do {\it zero-shot learning} which has been demonstrated convincingly by \newcite{gnmt16multi}. In that work, the authors built a single model that can do translation for 12 language pairs using the same sub-word vocabulary. Even more exciting, they can translate reasonably well for unseen language pairs at training time without using a pivot language. Ultimately, as what human does, it will be tremendously powerful if we can successfully learn from the data of all (sequence-to-sequence) tasks and construct a single model that can accomplish multiple goals, such as speech recognition and translation, at the same time. In this way, an intelligent system can take speech, for example in English, as input and produces on the fly a text translation, say in Urdu or Vietnamese, even though it has never seen any training data between the speech and text of that language pair.

{\it Semi-supervised} learning will also play a crucial role in the future of NMT systems. When mentioning about semi-supervised learning, I also imply the importance of unsupervised learning: any successful unsupervised learning model in text should provide a general form of language understanding that will be beneficial to downstream tasks that require supervision, e.g., \cite{dai15}.
In \secref{sec:6_1_exp}, I have also shown preliminary performance gains in translation by having auto-encoders or skip-thought training as unsupervised tasks in a multi-task setting. Such model, however, can only utilize a small amount of monolingual text, the data that exists in vast quantity. Human, in contrast, has the ability to learn a new language by first having some form of supervision such as a language teacher or a grammar book; afterwards, they can simply read books or material in that foreign language and keep improving their translation capabilities. Future NMT systems should be able to do so. In fact, recent approaches in {\it dual translation} models \cite{sennrich16mono,xia16}, which involve two back-and-forth translation models between a language pair, are heading towards that direction. In the former work, the authors simply use the reverse translation model to generate more parallel training data from the target-language monolingual text, which helps alleviating over-fitting. The latter work is closer to what I envision for the future: starting with 10\% of the bilingual data, the authors train both source-to-target and target-to-source models; then, through a Reinforcement Learning setting on monolingual data only, the two models help each other in improving their translation abilities. Using this approach, the dual-translation system can achieve comparable performance to NMT models trained on the full bilingual data. Ideally, we would want to keep learning from monolingual data forever and getting better and better over time.

%,cheng16
\subsection{Model Compression and Knowledge Distillation}
As we have discussed, the need for {\it model compression} is inevitable as mobile devices become ubiquitous nowadays and we want to make NMT models small enough to fit onto the device. In \secref{sec:nmt-compression}, I found it rather surprising that the parameters of NMT models can be pruned up to 80\% without any loss in performance as long as I retrain the pruned models. What I did, however, was only a proof-of-concept to show that there is a great redundancy in the parameter space of NMT models and it can be made smaller to fit onto mobile devices. I believe the future for NMT (and deep learning models in general) will involve about dealing efficiently with low-precision arithmetics \cite{courbariaux14,gupta15} and sparse models.

In parallel, the idea of {\it knowledge distillation} \cite{hinton15}, also proves to be of great importance in deep learning. What happens in practice is often, one can improve the system performance (sometimes by quite a lot) simply by training multiple models, an ensemble, and then averaging the predictions. Such a process is quite tedious and computationally expensive to deploy to users. The idea of distillation arises to address this problem by building a single neural network that can mimic the behavior of an ensemble. This turns out to work very well for NMT as demonstrated by \newcite{kim16distill}. Instead of trying to mimic an ensemble of models, they try build a smaller ({\it student}) network to learn from a larger ({\it teacher}) network. This not only speeds up inference time but also achieves the goal of making NMT models smaller. Additionally, they applied the absolute-value pruning technique that I proposed in \secref{sec:nmt-compression} to achieve further model compression for NMT, which is quite remarkable. I am looking forward to see knowledge distillation applied to not just one but over a variety of NMT models.

\subsection{Coherent and Style Translation Beyond Sentences}
So far, translation only happens at the {\it sentence} level: a paragraph or a document is split into multiple sentences, each of which is translated in isolation. This is, unfortunately, neither how human translates nor the way the meaning of texts is derived. Behind a sequence of sentences, there is often a high-level, sometimes complex, organization of thoughts, the {\it discourse structure} \cite{mann1988} which captures relationships among different text units such as comparison, elaboration, and evidence. Professional translators do not translate, for example a 4-sentence paragraph, using the exact number of sentences in the source; they can use more or less sentences depending on their understanding of the source text and how thoughts are presented in the target language. Early work \cite{marcu2000} hinted that modeling discourse structure is useful for machine translation, especially for distant language pairs such as English and Japanese.

The big picture here, in my opinion, is that future NMT systems should handle {\it coherence} and {\it style}, the two fundamental elements present in professional translations, on which current machine translation systems are missing. Coherence means translation beyond the sentence level and to achieve that, one might need to consider linguistic insights such as discourse structure analysis and coreference solution, just like how the attention model in NMT was motivated by word alignment notion in machine translation. 
Architecture-wise, I think models that can handle well very long sequences such as {\it hierarchical models} will be useful in maintaining translation coherence. In this paper \cite{li15}, I and other colleagues demonstrated the effectiveness of hierarchical models in constructing representations for long-text sequences such as paragraphs and documents in an autoencoder setting.
% Specifically, we first connect words to build up representations for sentences, then we connect sentences together to derive paragraph representations. 
Besides, hierarchical models have also proven its usefulness in other areas such as speech recognition \cite{chan16} and dialogue systems \cite{serban16}.

Maintaining style is an even harder problem. Not only does the translation system need to ensure coherence but it also has to recognize and preserve the tone of the source text, e.g., whether this is a formal text in business setting or if the text is informal and has some sense of humor, satire, etc. Accomplishing this will require deep understanding of languages such as recognizing idiomatic phrases, scare quotes, slang usages, and even implicit cultural referents. I believe insights from the area of dialogue systems will benefit style translation as we start seeing work in adding personalization to conversation dialogues \cite{li16,alrfou16}.

% work \cite{li16,alrfou16} has already begun in adding personalization to conversational dialogues.

Lastly, to make our progress measurable, evaluation datasets and proper automatic metrics will be tremendously useful as how BLEU \cite{Papineni02bleu} has helped advanced the field of machine translation. The authors in \cite{smith15} have put up a proposal for a coherence corpus in machine translation and I am looking forward to many more of such resources in the future.

% effective attention mechanism: hierarchical structures \cite{chan16}
% progress will not be made without a proper way of evaluation.



\subsection{Beyond Maximum Likelihood Estimation}
\cite{xia16,tu16}
