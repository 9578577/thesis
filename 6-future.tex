In previous chapters, my effort to improving neural machine
translation has been centered around enhancing the model architecture to address
different needs such as translating long sentences or coping with
complex vocabularies. In this chapter, I switch gear to examine more ``external''
aspects of NMT, which is also a way for me to take a quick peek into the future of NMT.
Specifically, I first examine in Section~\ref{sec:multi-task} how NMT can be improved by utilizing data from
not only the translation but also other tasks such as parsing, image caption
generation, and unsupervised learning. This is framed under the multi-task
setting which I believe is important for NMT future given a humongous amount of
data available in the world growing at an exponentially fast pace. The second
aspect that I examine is on making NMT models smaller, a topic of 
increasing importance as mobile devices become dominant. Specifically, in
Section~\ref{sec:nmt-compression}, I cast such
desiderata as a model compression problem in which I answer how much we can
reduce the sizes of NMT models without sacrifice in performance and reveal
interesting patterns in the parameter space of NMT. Lastly, in
Section~\ref{sec:outlook}, I highlight other future trends and potential
research directions for NMT.


\section{Multi-task Sequence to Sequence Learning}
\label{sec:multi-task}
\input{6-1_intro}

\subsection{Multi-task Sequence-to-Sequence Learning}
\label{subsec:multi}
\input{6-1_multi}

\subsection{Experiments}
\label{sec:exp}
\input{6-1_exp}

\subsection{Conclusion}
\label{sec:conclude}
\input{6-1_conclusion}

\section{Compression of NMT Models via Pruning}
\label{sec:nmt-compression}
\input{6-2_intro}

\subsection{Related Work}
\label{subsec:related}
\input{6-2_related}


\subsection{Our Approach}
\label{subsec:approach}
\input{6-2_approach}

\subsection{Experiments}
\label{subsec:exp}
\input{6-2_exp}

\subsection{Conclusion}
\label{subsec:conclusion}
\input{6-2_conclusion}

\section{Future Outlook}
\label{sec:outlook}
