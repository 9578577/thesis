Despite all of the advantages mentioned in the previous chapter, basic NMT systems are incapable of translating rare 
words because they have a fixed modest-sized vocabulary\footnote{ Due to the computationally intensive nature of the softmax, NMT systems often limit 
their vocabularies to be the top 30K-80K most frequent words in each language. 
%However, \newcite{jean15}
%has very recently proposed an efficient approximation to the softmax that allows
%for training NMTs with very large vocabularies. As discussed in Section~\ref{sec:nmt}, this technique is complementary to mine.
}
which forces them to use the \unksym{} symbol to 
represent the large number of out-of-vocabulary (OOV) words, as illustrated in Figure~\ref{f:sent_pair}.
Unsurprisingly, both \newcite{sutskever14} and \newcite{bog15} have
observed that sentences with many rare words tend to be translated much more poorly than sentences
containing mainly frequent words.
Standard phrase-based systems \cite{koehn2007moses,chiang07hiero,cer10phrasal,dyer10cdec}, 
on the other hand, do not suffer from the rare word 
problem to the same extent because they can support a much larger vocabulary, 
and because their use of explicit alignments
and phrase tables allows for memorizing the translations 
of even extremely rare words. 

\begin{figure*}
\resizebox{10cm}{!}{
\setlength{\unitlength}{1.1cm}
\begin{picture}(10, 2.7) %(-3,-2)
\put(0,2){{\it en}: The \unkword{ecotax} portico in \unkword{Pont-de-Buis} , \ldots [truncated] \ldots , was taken down on Thursday morning}
\put(0,1){{\it fr}: \mbox{} Le \unkword{portique} \unkword{\'{e}cotaxe} de \unkword{Pont-de-Buis} , \ldots [truncated] \ldots , a \'{e}t\'{e} \unkword{d\'{e}mont\'{e}} jeudi matin}
\put(0,0){{\it nn}: Le \unksym{} de \unksym{} \`{a} \unksym{} , \ldots [truncated] \ldots , a \'{e}t\'{e} pris le jeudi matin}
\put(1.7,1.3){\line(2,1){1.2}} % portico
\put(3.0,1.3){\line(-2,1){1.2}} % ecotax 
\put(5.2,1.3){\line(-1,2){0.3}} % Pont-de-Buis
\put(10.8,1.3){\line(-1,1){0.6}} % taken
\put(10.8,1.3){\line(1,3){0.2}} % down
\put(12,1.3){\line(3,2){0.9}} % Thursday
\put(13,1.3){\line(2,1){1.2}} % morning
\end{picture}
}
\caption[Example of the rare word problem]{{\bf Example of the rare word problem} -- An English source sentence ({\it en}), a human translation to French ({\it fr}), and a translation produced by one of my neural network systems ({\it nn}) before handling OOV words. I highlight \unkword{words} that are unknown to my model. 
The token \unksym{} indicates an OOV word. 
I also show a few important alignments between the pair of sentences. 
}
\label{f:sent_pair}
\end{figure*}

Motivated by the strengths of the standard phrase-based system, I 
propose and implement a novel approach to address the rare word problem of NMTs.
My approach annotates the training corpus with 
explicit alignment information that enables the NMT system to emit, for each OOV word, a
``pointer'' to its corresponding word in the source sentence. This
information is later utilized in a post-processing step that translates
the OOV words using a dictionary or with the identity translation, if no translation is found.


Experimental results confirm that this approach is effective. On the English to French WMT'14
translation task, this approach provides an improvement of
up to \bestunkimp{} BLEU points (if the vocabulary is relatively small) 
over an equivalent NMT system that does not use this technique.
Moreover, my system is the first NMT that outperforms the winner of a WMT'14 task.


