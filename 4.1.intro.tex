%Neural Machine Translation (NMT) can achieve state-of-the-art performances in
%large-scale translation tasks such as from English to French
%\cite{luong15} and
%English to German \cite{jean15}. NMT is appealing since it requires minimal
%domain knowledge and is conceptually simple. The model by \newcite{luong15} reads through all the source words until the end-of-sentence symbol \eos{} is reached. It then starts emitting one target word at a time, as illustrated in Figure~\ref{f:lstm}. NMT is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT \cite{Koehn:2003:SMT}.

While I have demonstrated in the previous chapter that Neural Machine Translation (NMT) can achieve state-of-the-art performance in
large-scale translation tasks such as from English to French, it is still challenging for NMT to handle long sentences as observed by \newcite{bog15}.
One effective way to address such problem is through the attention mechanism, which has gained popularity recently in
training neural networks, allowing models to learn alignments between different
modalities, e.g., between image objects and agent actions in the dynamic control
problem \cite{mnih14}, between speech frames and text in the speech recognition
task \cite{jan14},  or between visual features of a picture and its text
description in the image caption generation task \cite{xu15}. In the context of
NMT, \newcite{bog15} has successfully applied such attentional mechanism to
jointly translate and align words. To the best of my knowledge during the time of this work, there has not
been any other work exploring the use of attention-based architectures for NMT.

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth, clip=true, trim= 0 0 0 0]{img/4-lstm} % , angle=-90
\caption[Neural machine translation]{{\bf Neural machine translation} -- a stacking recurrent architecture for translating a source sequence \texttt{A B C D} into a target sequence \texttt{X Y Z}. Here, \eos{} marks the end of a sentence.
} 
\label{f:lstm}
\end{figure}

In this work, I design, with simplicity and effectiveness in mind, two novel
types of attention-based models: a {\it global} approach in which all source
words are attended and a {\it local} one whereby only a subset of source words
are considered at a time. The former approach resembles the model of
\cite{bog15} but is simpler architecturally. The latter can be viewed as an
interesting blend between the {\it hard} and {\it soft} attention models
proposed in \cite{xu15}: it is computationally less expensive than the
global model or the soft attention; at the same time, unlike the hard attention,
the local attention is
differentiable almost everywhere, making it easier to implement and
train.\footnote{There is a recent work by \newcite{draw15}, which is very
similar to my local attention and applied to the image generation task.
However, as I detail later, my model is much simpler and can achieve good performance for NMT.} Besides, I also examine various
alignment functions for my attention-based models.

Following \cite{sutskever14,luong15}, I use the stacking LSTM architecture for our NMT systems, as illustrated in Figure~\ref{f:lstm}, together with the LSTM unit defined in \cite{zaremba14}.
The experimental results demonstrate that both of my approaches are
effective in the WMT translation tasks between English and German in  both
directions. My attentional models yield a boost of up to \attngain{} BLEU over
non-attentional systems which already incorporate known techniques such as
dropout. For English to German translation, I achieve new state-of-the-art
(SOTA)
results for both WMT'14 and WMT'15, outperforming previous SOTA systems, backed by
NMT models and $n$-gram LM rerankers, by more than 1.0 BLEU. I conduct
extensive analysis to evaluate my models in terms of learning, the ability to
handle long sentences, choices of attentional architectures, alignment quality, and translation
outputs. 
