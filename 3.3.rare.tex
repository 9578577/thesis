Despite the relatively large amount of work done on pure neural machine translation systems, 
there has been no work addressing the OOV problem in NMT systems, 
with the notable exception of \newcite{jean15}'s work mentioned earlier. 

We propose to address the rare word problem by training the NMT system
to track the origins of the unknown words in the target sentences.  If
we knew the source word responsible for each unknown target word, we could introduce
a post-processing step that would replace each \unksym{} in the system's output
with a translation of its source word, using 
either a dictionary or the identity translation.  For example, in
Figure~\ref{f:sent_pair}, if the model knows that the second unknown token 
in the NMT (line {\it nn}) originates from the source
word \texttt{ecotax}, it can perform a word dictionary lookup to
replace that unknown token by \texttt{\'{e}cotaxe}. Similarly, an
identity translation of the source word \texttt{Pont-de-Buis} can be
applied to the third unknown token.

We present three annotation strategies that can easily be applied to any NMT system \cite{kal13,sutskever14,cho14}. 
We treat the NMT system as a black box and train it on a corpus annotated by one of the models below. 
First, the alignments are produced with an unsupervised aligner. 
Next, we use the alignment links to construct a word dictionary that will 
be used for the word translations in the post-processing step.\footnote{When a source word has multiple translations, we use the translation with the highest probability. These translation probabilities are estimated from the unsupervised alignment links. When constructing the dictionary from these alignment links, we add a word pair to the dictionary only if its alignment count exceeds 100.}
If a word does not appear in our dictionary, then we apply the identity translation.

The first few words of the sentence pair in Figure~\ref{f:sent_pair} (lines {\it en}
and {\it fr}) illustrate our models. 

\subsection{Copyable Model}
\label{subsec:copyable}
\begin{figure}
\resizebox{10.5cm}{!}{
\setlength{\unitlength}{1cm}
\begin{picture}(10, 1) %(-3,-2)
\put(0.5,0.7){en: The \unkcopy{1} portico in \unkcopy{2} \ldots}
\put(0.5,0){fr: \mbox{} Le \unknull{} \unkcopy{1} de \unkcopy{2} \ldots}
\end{picture}
}
\caption[Copyable Model]{ {\bf Copyable Model} -- an annotated example with two 
types of unknown tokens: ``copyable'' \unktext{n} and null \unknull{}.}
\label{f:copyable}
\end{figure}

In this approach, we introduce multiple tokens to represent the various unknown words in the 
source and in the target language, as opposed to using only one \unksym{} token. 
We annotate the OOV words in the source sentence
with \unktext{1}, \unktext{2}, \unktext{3}, in that order,
while assigning repeating unknown words identical tokens. 
The annotation of the unknown words in the target language is slightly more elaborate: (a) each 
unknown target word that is aligned to an unknown source word
is assigned the same unknown token (hence, the ``copy'' model) and 
(b) an unknown target word that has no 
alignment or that is aligned with a known word uses the special null token \unknull{}. 
See Figure~\ref{f:copyable} for an example.  This annotation enables us to 
translate every non-null unknown token.

\subsection{Positional All Model (PosAll)}
The copyable model is limited by its inability to translate unknown 
target words that are aligned to \emph{known} words in the source sentence, such as the pair of 
words, ``portico'' and ``portique'', in our running example. 
The former word is known on the source sentence; whereas latter is not, so it is labelled with \unknull{}.
This happens often since the source vocabularies of our models tend to be much 
larger than the target vocabulary since a large source vocabulary is cheap.
This limitation motivated us to develop an annotation model that includes the complete 
alignments between the source and the target sentences, which is straightforward to obtain
 since the complete alignments are available at training time.  

Specifically, we return to using only a single universal \unksym{} token. 
However, on the target side, 
we insert a positional token \postext{d} after every word. Here, $d$ indicates a relative position 
($d=-7,\ldots,-1,0,1,\ldots,7$) to denote that a target word at position $j$ is aligned 
to a source word 
at position $i=j-d$. Aligned words that are too far apart are considered unaligned, and 
unaligned words rae annotated
with a null token \postext{n}. Our annotation is illustrated in 
Figure~\ref{f:pos_all}.

\begin{figure}
\resizebox{10.5cm}{!}{
\setlength{\unitlength}{1cm}
\begin{picture}(10, 1) %(-3,-2)
\put(0,0.7){en: The \unksym{} portico in \unksym{} \ldots} 
\put(0,0){fr: \mbox{} Le \pos{0} \unksym{} \pos{-1} \unksym{} \pos{1} de \posnull{} \unksym{} \pos{-1} \ldots}
\end{picture}
}
\caption[Positional All Model]{ {\bf Positional All Model} -- an example of the PosAll model. Each word is followed by the relative positional tokens \postext{d} or the null token \posnull{}. }
\label{f:pos_all}
\end{figure}

\subsection{Positional Unknown Model (PosUnk)}

The main weakness of the PosAll model is that it doubles the length of the target sentence. This
makes learning more difficult and slows the speed of parameter updates by a factor of two.
However, given that our post-processing step is concerned only with the alignments of the unknown words,
so it is more sensible to only annotate the unknown words. 
This motivates our {\it positional unknown} model which uses \unkpostext{d} 
tokens (for $d$ in $-7,\ldots,7$ or $\emptyset$) to simultaneously 
denote (a) the fact that a word is unknown and (b) its relative position $d$ with respect to its aligned source word. 
Like the PosAll model, we use the symbol \unkpos{\emptyset} for unknown target words that do not have an alignment. 
We use the universal \unksym{} for all unknown tokens in the source language. See Figure~\ref{f:pos_unk} for an annotated example.

\begin{figure}[tbh!]
\resizebox{10.5cm}{!}{
\setlength{\unitlength}{1cm}
\begin{picture}(10, 1) %(-3,-2)
\put(0,0.7){en: The \unksym{} portico in \unksym{} \ldots} 
\put(0,0){fr: \mbox{} Le \unkpos{1} \unkpos{-1} de \unkpos{1} \ldots}
\end{picture}
}
\caption[Positional Unknown Model]{ {\bf Positional Unknown Model} -- 
an example of the PosUnk model: only aligned unknown words are annotated with the \unkpostext{d} tokens.}
\label{f:pos_unk}
\end{figure}

It is possible that despite its slower speed, the PosAll model will learn better alignments because 
it is trained on many more examples of words and their alignments. 
However, we show that this is not the case (see $\S$\ref{subsec:rare_model_compare}).


